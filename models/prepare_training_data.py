# -*- coding: utf-8 -*-
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ training data –¥–ª—è ML –º–æ–¥–µ–ª–∏
–ú–∞—Ä–∂–∏—Ä—É–µ—Ç features —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pickle

class TrainingDataPreparer:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏"""

    def __init__(self):
        self.scaler = StandardScaler()

    def load_features(self, features_path):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ features"""
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é features –∏–∑ {features_path}...")
        df_features = pd.read_csv(features_path)
        df_features['timestamp'] = pd.to_datetime(df_features['timestamp'])

        # ‚úÖ –ù–û–í–û–ï: –û—á–∏—Å—Ç–∏—Ç—å inf/nan —Å—Ä–∞–∑—É
        print(f"   üßπ –û—á–∏—Å—Ç–∫–∞ inf/nan –≤ features...")
        df_features = df_features.replace([np.inf, -np.inf], np.nan)

        # –ó–∞–º–µ–Ω–∏—Ç—å NaN –Ω–∞ median (–∏–ª–∏ 0 –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö features)
        for col in df_features.select_dtypes(include=[np.number]).columns:
            if df_features[col].isna().sum() > 0:
                median_val = df_features[col].median()
                df_features[col].fillna(median_val if not np.isnan(median_val) else 0, inplace=True)

        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_features)} —Å—Ç—Ä–æ–∫ (cleaned)")
        return df_features

    def load_backtest_results(self, backtest_csv):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã backtesta"""
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã backtesta –∏–∑ {backtest_csv}...")
        df_trades = pd.read_csv(backtest_csv)
        df_trades['entry_time'] = pd.to_datetime(df_trades['entry_time'])
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_trades)} —Å–¥–µ–ª–æ–∫")
        return df_trades

    def merge_features_with_trades(self, df_features, df_trades):
        """–ú–∞—Ä–∂–∏—Ä–æ–≤–∞—Ç—å features —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–æ—Ä–≥–æ–≤–ª–∏"""
        print(f"\nüîÄ –ú–∞—Ä–∂–∏—Ä–æ–≤–∞–Ω–∏–µ features —Å —Ç–æ—Ä–≥–æ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏...")

        merged_list = []

        for idx, trade in df_trades.iterrows():
            entry_time = trade['entry_time']

            # –ù–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à–∏–π row –ø–µ—Ä–µ–¥ –≤—Ö–æ–¥–æ–º (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 10 –º–∏–Ω—É—Ç)
            time_mask = (df_features['timestamp'] <= entry_time) & \
                       (df_features['timestamp'] >= entry_time - pd.Timedelta(minutes=10))

            matching_rows = df_features[time_mask]

            if len(matching_rows) > 0:
                # –í–∑—è—Ç—å –±–ª–∏–∂–∞–π—à–∏–π
                closest_idx = (matching_rows['timestamp'] - entry_time).abs().idxmin()
                feature_row = df_features.loc[closest_idx].to_dict()

                feature_row['pnl'] = trade['pnl']
                feature_row['pnl_pct'] = trade['pnl_pct']
                feature_row['label'] = 1 if trade['pnl'] > 0 else 0
                feature_row['entry_time'] = entry_time

                merged_list.append(feature_row)

        df_merged = pd.DataFrame(merged_list)
        print(f"   ‚úÖ –ú–∞—Ä–∂–∏—Ä–æ–≤–∞–Ω–æ {len(df_merged)} trade samples")

        return df_merged

    def prepare_ml_data(self, df_merged, exclude_cols=['timestamp', 'pnl', 'pnl_pct', 'label', 'entry_time']):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å X (features) –∏ y (labels)"""
        print(f"\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ ML data...")

        if len(df_merged) == 0:
            raise ValueError("‚ùå No samples to process!")

        # –í—ã–±—Ä–∞—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ features
        feature_cols = [col for col in df_merged.columns
                       if col not in exclude_cols
                       and df_merged[col].dtype in ['float64', 'int64', 'float32', 'int32']]

        print(f"   üìä –í—Å–µ–≥–æ features: {len(feature_cols)}")
        print(f"   üìä Samples before cleaning: {len(df_merged)}")

        # –°–æ–∑–¥–∞—Ç—å —á–∏—Å—Ç—ã–π DataFrame
        df_clean = df_merged[feature_cols + ['label']].copy()

        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å inf/nan
        print(f"   üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ inf/nan...")
        for col in feature_cols:
            inf_count = np.isinf(df_clean[col]).sum()
            nan_count = df_clean[col].isna().sum()
            if inf_count > 0 or nan_count > 0:
                print(f"      ‚ö†Ô∏è {col}: {inf_count} inf, {nan_count} nan")

        # –£–¥–∞–ª–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é NaN —Å—Ç—Ä–æ–∫–∏
        df_clean = df_clean.dropna(subset=['label'])

        # –ó–∞–º–µ–Ω–∏—Ç—å remaining inf/nan
        df_clean = df_clean.replace([np.inf, -np.inf], np.nan)

        for col in feature_cols:
            if df_clean[col].isna().any():
                median_val = df_clean[col].median()
                df_clean[col].fillna(median_val if not np.isnan(median_val) else 0, inplace=True)

        print(f"   ‚úÖ Samples after cleaning: {len(df_clean)}")

        if len(df_clean) == 0:
            raise ValueError("‚ùå All samples removed after cleaning!")

        X = df_clean[feature_cols].values
        y = df_clean['label'].values

        print(f"\n   üìä Final data:")
        print(f"      Features: {X.shape[1]}")
        print(f"      Samples: {X.shape[0]}")
        print(f"      Win Rate: {y.mean()*100:.1f}%")
        print(f"      Wins: {y.sum()} | Losses: {len(y) - y.sum()}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –Ω–µ—Ç inf/nan –≤ X
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            print("   ‚ö†Ô∏è WARNING: Still has inf/nan, replacing with 0...")
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
        X_scaled = self.scaler.fit_transform(X)

        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–µ –¥–µ–ª–∞–µ–º split –µ—Å–ª–∏ samples < 20
        if len(X_scaled) < 20:
            print(f"\n   ‚ö†Ô∏è Small dataset ({len(X_scaled)} samples)")
            print(f"   Using all data for training (no validation split)")
            return X_scaled, X_scaled, y, y, feature_cols

        # Split –Ω–∞ train/val
        try:
            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y
            )

            print(f"\n   Train set: {X_train.shape[0]} samples")
            print(f"   Val set: {X_val.shape[0]} samples")

            return X_train, X_val, y_train, y_val, feature_cols

        except ValueError as e:
            print(f"\n   ‚ö†Ô∏è Can't stratify (too few samples): {e}")
            print(f"   Using simple split...")

            X_train, X_val, y_train, y_val = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )

            return X_train, X_val, y_train, y_val, feature_cols

    def save_scaler(self, path='models/scaler.pkl'):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å scaler"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        pickle.dump(self.scaler, open(path, 'wb'))
        print(f"\nüíæ Scaler saved: {path}")

def main():
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"""
    preparer = TrainingDataPreparer()

    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å features
    df_features = preparer.load_features(
        "data/ml_training/features/BTCUSDT_features.csv"
    )

    # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å results backtesta
    df_trades = preparer.load_backtest_results(
          "tests/results/backtest_5min_ml_20251104_191414.csv"
    )

    # 3. –ú–∞—Ä–∂–∏—Ä–æ–≤–∞—Ç—å
    df_merged = preparer.merge_features_with_trades(df_features, df_trades)

    # 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å X –∏ y
    X_train, X_val, y_train, y_val, feature_cols = preparer.prepare_ml_data(df_merged)

    # 5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å scaler
    preparer.save_scaler()

    # 6. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å training data
    os.makedirs("data/ml_training/training_data", exist_ok=True)

    np.save("data/ml_training/training_data/X_train.npy", X_train)
    np.save("data/ml_training/training_data/X_val.npy", X_val)
    np.save("data/ml_training/training_data/y_train.npy", y_train)
    np.save("data/ml_training/training_data/y_val.npy", y_val)

    with open("data/ml_training/training_data/feature_cols.pkl", 'wb') as f:
        pickle.dump(feature_cols, f)

    print(f"\n‚úÖ Training data saved!")
    print(f"   X_train shape: {X_train.shape}")
    print(f"   X_val shape: {X_val.shape}")

if __name__ == "__main__":
    main()
